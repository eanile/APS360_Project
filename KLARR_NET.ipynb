{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KLARR-NET.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.6 64-bit ('venv')",
      "metadata": {
        "interpreter": {
          "hash": "5f88ab50eb7194516fb24d43ad7b5ea8fdfbd265589dc4d764948d225392ac98"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLEeOTfOx9mH"
      },
      "source": [
        "# Baseline Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd-7ueo0yAww"
      },
      "source": [
        "Below we have the baseline model using an SVM network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ-RO9K_ah7J"
      },
      "source": [
        "\"\"\"\n",
        "Baseline Model - SVM\n",
        "\"\"\"\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn import svm, metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(0)\n",
        "RANDOM_NUM = np.random.randint(1000)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "source": [
        "DATA_PATH = \"KLARR_NET_data/fer_2013\"\n",
        "\n",
        "IMG_WIDTH = 48\n",
        "IMG_HEIGHT = 48\n",
        "\n",
        "def create_dataset(img_folder):\n",
        "    \"\"\" Creates a dataset of the images in the given folder and returns the\n",
        "    data and labels array.\n",
        "    \"\"\"\n",
        "    img_data_array = []\n",
        "    class_name = []\n",
        "   \n",
        "    for class_num, dir in enumerate(os.listdir(img_folder)):\n",
        "        print(f\"Class Num: {class_num}, Directory: {dir}\")\n",
        "\n",
        "        for i, filename in enumerate(os.listdir(os.path.join(img_folder, dir))):\n",
        "            if i % 1000 == 0:\n",
        "                print(i)\n",
        "\n",
        "            # Read the image in grayscale\n",
        "            image_path = os.path.join(img_folder, dir, filename)\n",
        "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "            image = cv2.resize(\n",
        "                image,\n",
        "                (IMG_HEIGHT, IMG_WIDTH),\n",
        "                interpolation=cv2.INTER_AREA\n",
        "            )\n",
        "\n",
        "            # Convert to numpy and normalize between 0 and 1\n",
        "            image = np.array(image).astype('float32').flatten() / 255\n",
        "\n",
        "            img_data_array.append(image)\n",
        "            class_name.append(class_num)\n",
        "\n",
        "    return np.array(img_data_array), np.array(class_name)\n",
        "\n",
        "X_train, y_train = create_dataset(DATA_PATH)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    train_size=0.7,\n",
        "    random_state=RANDOM_NUM)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "j-YM58k1keSH"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Num: 0, Directory: angry\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "Class Num: 1, Directory: fear\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "Class Num: 2, Directory: happy\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "Class Num: 3, Directory: neutral\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "Class Num: 4, Directory: sad\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "Class Num: 5, Directory: surprise\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Model Training Set Size: 22408\nClass: angry, Number of Classes: 3467\nClass: fear, Number of Classes: 3536\nClass: happy, Number of Classes: 3958\nClass: neutral, Number of Classes: 4365\nClass: sad, Number of Classes: 4289\nClass: surprise, Number of Classes: 2793\n\nBaseline Model Test Set Size: 9604\nClass: angry, Number of Classes: 1450\nClass: fear, Number of Classes: 1545\nClass: happy, Number of Classes: 1827\nClass: neutral, Number of Classes: 1821\nClass: sad, Number of Classes: 1773\nClass: surprise, Number of Classes: 1188\n"
          ]
        }
      ],
      "source": [
        "CLASSES = ['angry', 'fear', 'happy' ,'neutral' ,'sad' ,'surprise']\n",
        "\n",
        "def print_class_distribution_in_dataset(dataset_labels):\n",
        "    num_classes = [0] * len(CLASSES)\n",
        "    for label in dataset_labels:\n",
        "        num_classes[label] += 1\n",
        "    \n",
        "    for i, num_of_class in enumerate(num_classes):\n",
        "        print(f\"Class: {CLASSES[i]}, Number of Classes: {num_of_class}\")\n",
        "\n",
        "print(f\"Baseline Model Training Set Size: {len(X_train)}\")\n",
        "print_class_distribution_in_dataset(y_train)\n",
        "\n",
        "print(f\"\\nBaseline Model Test Set Size: {len(X_test)}\")\n",
        "print_class_distribution_in_dataset(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT3NiwRLr8D6"
      },
      "source": [
        "# Instantiate the SVM classifier\n",
        "# svm_classifier = svm.SVC(gamma=0.001)\n",
        "svm_classifier = svm.SVC(\n",
        "    kernel='linear',\n",
        "    C=1,\n",
        "    decision_function_shape='ovo',\n",
        ")\n",
        "\n",
        "# Fit to the training data\n",
        "svm_classifier.fit(X_train, y_train)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1, decision_function_shape='ovo', kernel='linear')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WaywokHsWPd"
      },
      "source": [
        "# y_pred = svm_classifier.predict(X_test)\n",
        "# classification_report = metrics.classification_report(y_test, y_pred)\n",
        "# confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# baseline_test_acc = svm_classifier.score(X_test, y_test)\n",
        "print(f\"Baseline Model Test Accuracy: {baseline_test_acc}\")\n",
        "\n",
        "print(f\"Classification report for classifier {svm_classifier}:\")\n",
        "print(f\"\\n{classification_report}\")\n",
        "\n",
        "print(f\"\\nConfusion matrix:\\n\\n{confusion_matrix}\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Model Test Accuracy: 0.3263223656809663\nClassification report for classifier SVC(C=1, decision_function_shape='ovo', kernel='linear'):\n\n              precision    recall  f1-score   support\n\n           0       0.23      0.25      0.24      1450\n           1       0.25      0.24      0.24      1545\n           2       0.40      0.39      0.40      1827\n           3       0.33      0.36      0.35      1821\n           4       0.29      0.29      0.29      1773\n           5       0.49      0.43      0.46      1188\n\n    accuracy                           0.33      9604\n   macro avg       0.33      0.33      0.33      9604\nweighted avg       0.33      0.33      0.33      9604\n\n\nConfusion matrix:\n\n[[359 217 250 276 266  82]\n [229 368 212 280 282 174]\n [279 217 720 262 264  85]\n [247 248 223 651 343 109]\n [321 239 268 334 522  89]\n [127 176 112 141 118 514]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['models/baseline_svm_1.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# # Save model parameters so we can load it later\n",
        "# from joblib import dump, load\n",
        "# from pathlib import Path\n",
        "\n",
        "# BASELINE_MODEL_LOCATION = \"models/baseline_svm_1.joblib\"\n",
        "\n",
        "# Path(\"models\").mkdir(parents=True, exist_ok=True)\n",
        "# dump(svm_classifier, BASELINE_MODEL_LOCATION)\n",
        "\n",
        "# # To load the model, uncomment this line\n",
        "# clf = load(BASELINE_MODEL_LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuOOOrZux5Js"
      },
      "source": [
        "# CNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcH3H5A3tEaj"
      },
      "source": [
        "\"\"\"\n",
        "CNN model Implementation\n",
        "\"\"\"\n",
        "\n",
        "import os \n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torch.utils.data as data\n",
        "\n",
        "TRAIN_PATH = \"\"\n",
        "TEST_PATH =  \"\"\n",
        "VALIDATION_PATH =  \"\"\n",
        "\n",
        "TRANSFORM_IMG = transforms.Compose([\n",
        "    #transforms.Grayscale(),\n",
        "    transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "train_data = torchvision.datasets.ImageFolder(root=TRAIN_PATH, transform=TRANSFORM_IMG)\n",
        "print('Number of Train Images : ', len(train_data))\n",
        "\n",
        "test_data = torchvision.datasets.ImageFolder(root=TEST_PATH, transform=TRANSFORM_IMG)\n",
        "print('Number of Test Images : ', len(test_data))\n",
        "\n",
        "validation_data = torchvision.datasets.ImageFolder(root=VALIDATION_PATH, transform=TRANSFORM_IMG)\n",
        "print('Number of Validation Images : ', len(validation_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQxK5KnDtwtl"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim #for gradient descent\n",
        "\n",
        "class KLARR_NET(nn.Module):\n",
        "  def __init__(self):\n",
        "        torch.manual_seed(1000)\n",
        "        super(ASLClassifer, self).__init__()\n",
        "        self.name = 'ASLClassifier'\n",
        "        self.conv1 = nn.Conv2d(3, 5, 5) # TODO : make sure this input size fits with the VGG16 feature size\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(5, 10, 5)\n",
        "        self.conv3 = nn.Conv2d(10, 10, 5)\n",
        "        self.fc1 = nn.Linear(10*24*24, 72)\n",
        "        self.fc2 = nn.Linear(72, 9)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.pool(F.relu(self.conv1(x)))\n",
        "      x = self.pool(F.relu(self.conv2(x)))\n",
        "      x = self.pool(F.relu(self.conv3(x)))\n",
        "      x = x.view(-1, 10*24*24)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = self.fc2(x)\n",
        "\n",
        "      x = x.squeeze(1) # Flatten to [batch_size]\n",
        "\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK5JrGgRvVCG"
      },
      "source": [
        "def get_model_name(name, batch_size, learning_rate, epoch):\n",
        "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object containing the hyperparameters\n",
        "    Returns:\n",
        "        path: A string with the hyperparameter name and value concatenated\n",
        "    \"\"\"\n",
        "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
        "                                                   batch_size,\n",
        "                                                   learning_rate,\n",
        "                                                   epoch)\n",
        "    return path\n",
        "\n",
        "def get_accuracy(model, data_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for img, label in iter(data_loader):\n",
        "      out = model(img)\n",
        "      pred = out.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(label.view_as(pred)).sum().item()\n",
        "      total += img.shape[0]\n",
        "    #print('Accuracy :', correct/total)\n",
        "    return correct / total\n",
        "\n",
        "def train(model, train_data, val_data, batch_size, learning_rate, iterations):\n",
        "\n",
        "  # torch.manual_seed(1000)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  iters, losses, train_acc, val_acc = [], [], [], []\n",
        "  val_loader = data.DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "  train_loader = data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "  n = 0\n",
        "  for iteraion in range(interations):\n",
        "    print('ITERATION : ', epoch)\n",
        "    for img, label in iter(train_loader):\n",
        "      out = model(img)\n",
        "\n",
        "      loss = criterion(out, label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    iters.append(n)\n",
        "    losses.append(float(loss) / batch_size)\n",
        "    train_acc.append(get_accuracy(model, train_loader))\n",
        "    val_acc.append(get_accuracy(model, val_loader))\n",
        "    n += 1\n",
        "\n",
        "    # Save the current model (checkpoint) to a file\n",
        "    # model_path = get_model_name(model.name, batch_size, learning_rate, epoch)\n",
        "    # torch.save(model.state_dict(), model_path)\n",
        "    \n",
        "  # Plotting\n",
        "  plt.title(\"Training Curve\")\n",
        "  plt.plot(iters, losses, label=\"Train\")\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Training Curve\")\n",
        "  plt.plot(iters, train_acc, label=\"Train\")\n",
        "  plt.plot(iters, val_acc, label=\"Validation\")\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Training Accuracy\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "  print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AXM07gfxXex"
      },
      "source": [
        "import torchvision.models\n",
        "alexnet = torchvision.models.vgg16(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df-2XYt6yQaY"
      },
      "source": [
        "Train_PATH = \"\"\n",
        "validation_PATH = \"\"\n",
        "Test_PATH = \"\"\n",
        "\n",
        "classes= ['angry','fear','happy','neutral','sad','surprise']\n",
        "\n",
        "train_data_loader = data.DataLoader(train_data, batch_size=1, shuffle=False,  num_workers=0)\n",
        "\n",
        "test_data_loader = data.DataLoader(test_data, batch_size=1, shuffle=False,  num_workers=0)\n",
        "\n",
        "validation_data_loader = data.DataLoader(validation_data, batch_size=1, shuffle=False,  num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBrvFBVnyn1U"
      },
      "source": [
        "\"\"\"\n",
        "Code to import the vgg16 features of our images into separate image folders in google drive\n",
        "\"\"\"\n",
        "\n",
        "def save_tensor(feature, folder_name, i):\n",
        "  if not os.path.isdir(folder_name):\n",
        "    os.mkdir(folder_name)\n",
        "  filename = folder_name + '/' + str(i) + '.tensor'\n",
        "  if not os.path.isfile(filename):\n",
        "    torch.save(feature.squeeze(0), folder_name + '/' + str(i) + '.tensor')\n",
        "\n",
        "\n",
        "iteration_no = 0 \n",
        "for img, label in train_data_loader:\n",
        "  features = alexnet.features(img)\n",
        "  features_tensor = torch.from_numpy(features.detach().numpy())\n",
        "  folder_name = Train_PATH  + str(classes[label])\n",
        "\n",
        "  save_tensor(features_tensor, folder_name, iteration_no)\n",
        "  iteration_no += 1\n",
        "\n",
        "iteration_no = 0 \n",
        "for img, label in test_data_loader:\n",
        "  features = alexnet.features(img)\n",
        "  features_tensor = torch.from_numpy(features.detach().numpy())\n",
        "  folder_name = Test_PATH + str(classes[label])\n",
        "  \n",
        "  save_tensor(features_tensor, folder_name, iteration_no)\n",
        "  iteration_no += 1\n",
        "\n",
        "iteration_no = 0\n",
        "for img, label in validation_data_loader:\n",
        "  features = alexnet.features(img)\n",
        "  features_tensor = torch.from_numpy(features.detach().numpy())\n",
        "  folder_name = validation_PATH  + str(classes[label])\n",
        "\n",
        "  save_tensor(features_tensor, folder_name, iteration_no)\n",
        "  iteration_no += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq_0y0iuzMDt"
      },
      "source": [
        "# Import features from location on Google Drive\n",
        "feature_train_set = torchvision.datasets.DatasetFolder(Train_PATH, loader=torch.load, extensions=('.tensor'))\n",
        "feature_val_set = torchvision.datasets.DatasetFolder(validation_PATH, loader=torch.load, extensions=('.tensor'))\n",
        "feature_test_set = torchvision.datasets.DatasetFolder(Test_PATH, loader=torch.load, extensions=('.tensor'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}